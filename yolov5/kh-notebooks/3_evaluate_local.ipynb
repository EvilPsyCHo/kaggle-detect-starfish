{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedfff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Copy from implement by \n",
    "# https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/290083\n",
    "# https://www.kaggle.com/bamps53/competition-metric-implementation?scriptVersionId=81087805\n",
    "\n",
    "# Yolo infer modified from \n",
    "# https://www.kaggle.com/awsaf49/great-barrier-reef-yolov5-infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102951b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 30, 30\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "from ast import literal_eval\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('../input/tensorflow-great-barrier-reef')\n",
    "import torch\n",
    "from PIL import Image\n",
    "import ast\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3d8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(row):\n",
    "    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n",
    "    return row\n",
    "\n",
    "def voc2yolo(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    voc  => [x1, y1, x2, y1]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    w = bboxes[..., 2] - bboxes[..., 0]\n",
    "    h = bboxes[..., 3] - bboxes[..., 1]\n",
    "    \n",
    "    bboxes[..., 0] = bboxes[..., 0] + w/2\n",
    "    bboxes[..., 1] = bboxes[..., 1] + h/2\n",
    "    bboxes[..., 2] = w\n",
    "    bboxes[..., 3] = h\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2voc(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    voc  => [x1, y1, x2, y1]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def coco2yolo(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # normolizinig\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    # converstion (xmin, ymin) => (xmid, ymid)\n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2coco(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # denormalizing\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    # converstion (xmid, ymid) => (xmin, ymin) \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def voc2coco(bboxes, image_height=720, image_width=1280):\n",
    "    bboxes  = voc2yolo(bboxes, image_height, image_width)\n",
    "    bboxes  = yolo2coco(bboxes, image_height, image_width)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, \n",
    "                bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n",
    "     \n",
    "    image = img.copy()\n",
    "    show_classes = classes if show_classes is None else show_classes\n",
    "    colors = (0, 255 ,0) if colors is None else colors\n",
    "    \n",
    "    if bbox_format == 'yolo':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:\n",
    "            \n",
    "                x1 = round(float(bbox[0])*image.shape[1])\n",
    "                y1 = round(float(bbox[1])*image.shape[0])\n",
    "                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n",
    "                h  = round(float(bbox[3])*image.shape[0]/2)\n",
    "\n",
    "                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(get_label(cls)),\n",
    "                             line_thickness = line_thickness)\n",
    "            \n",
    "    elif bbox_format == 'coco':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:            \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                w  = int(round(bbox[2]))\n",
    "                h  = int(round(bbox[3]))\n",
    "\n",
    "                voc_bbox = (x1, y1, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness)\n",
    "\n",
    "    elif bbox_format == 'voc_pascal':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes: \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                x2 = int(round(bbox[2]))\n",
    "                y2 = int(round(bbox[3]))\n",
    "                voc_bbox = (x1, y1, x2, y2)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness)\n",
    "    else:\n",
    "        raise ValueError('wrong bbox format')\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes\n",
    "\n",
    "def get_imgsize(row):\n",
    "    row['width'], row['height'] = imagesize.get(row['image_path'])\n",
    "    return row\n",
    "\n",
    "np.random.seed(32)\n",
    "colors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n",
    "          for idx in range(1)]\n",
    "\n",
    "\n",
    "def decode_annotations(annotaitons_str):\n",
    "    \"\"\"decode annotations in string to list of dict\"\"\"\n",
    "    return literal_eval(annotaitons_str)\n",
    "\n",
    "def load_image_with_annotations(video_id, video_frame, image_dir, annotaitons_str):\n",
    "    img = load_image(video_id, video_frame, image_dir)\n",
    "    annotations = decode_annotations(annotaitons_str)\n",
    "    if len(annotations) > 0:\n",
    "        for ann in annotations:\n",
    "            cv2.rectangle(img, (ann['x'], ann['y']),\n",
    "                (ann['x'] + ann['width'], ann['y'] + ann['height']),\n",
    "                (255, 0, 0), thickness=2,)\n",
    "    return img\n",
    "\n",
    "def draw_predictions(img, pred_bboxes):\n",
    "    img = img.copy()\n",
    "    if len(pred_bboxes) > 0:\n",
    "        for bbox in pred_bboxes:\n",
    "            conf = bbox[0]\n",
    "            x, y, w, h = bbox[1:].round().astype(int)\n",
    "            cv2.rectangle(img, (x, y),\n",
    "                (x+w, y+h),\n",
    "                (0, 255, 255), thickness=2,)\n",
    "            cv2.putText(\n",
    "                img,\n",
    "                f\"{conf:.2}\",\n",
    "                (x, max(0, y-5)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 0, 255),\n",
    "                thickness=1,\n",
    "            )\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42955d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(bboxes1, bboxes2, bbox_mode='xywh'):\n",
    "    assert len(bboxes1.shape) == 2 and bboxes1.shape[1] == 4\n",
    "    assert len(bboxes2.shape) == 2 and bboxes2.shape[1] == 4\n",
    "    \n",
    "    bboxes1 = bboxes1.copy()\n",
    "    bboxes2 = bboxes2.copy()\n",
    "    \n",
    "    if bbox_mode == 'xywh':\n",
    "        bboxes1[:, 2:] += bboxes1[:, :2]\n",
    "        bboxes2[:, 2:] += bboxes2[:, :2]\n",
    "\n",
    "    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n",
    "    xA = np.maximum(x11, np.transpose(x21))\n",
    "    yA = np.maximum(y11, np.transpose(y21))\n",
    "    xB = np.minimum(x12, np.transpose(x22))\n",
    "    yB = np.minimum(y12, np.transpose(y22))\n",
    "    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n",
    "    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n",
    "    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n",
    "    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n",
    "    return iou\n",
    "\n",
    "def f_beta(tp, fp, fn, beta=2):\n",
    "    return (1+beta**2)*tp / ((1+beta**2)*tp + beta**2*fn+fp)\n",
    "\n",
    "def calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th, verbose=False):\n",
    "    gt_bboxes = gt_bboxes.copy()\n",
    "    pred_bboxes = pred_bboxes.copy()\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for k, pred_bbox in enumerate(pred_bboxes): # fixed in ver.7\n",
    "        ious = calc_iou(gt_bboxes, pred_bbox[None, 1:])\n",
    "        max_iou = ious.max()\n",
    "        if max_iou > iou_th:\n",
    "            tp += 1\n",
    "            gt_bboxes = np.delete(gt_bboxes, ious.argmax(), axis=0)\n",
    "        else:\n",
    "            fp += 1\n",
    "        if len(gt_bboxes) == 0:\n",
    "            fp += len(pred_bboxes) - (k + 1) # fix in ver.7\n",
    "            break\n",
    "\n",
    "    fn = len(gt_bboxes)\n",
    "    return tp, fp, fn\n",
    "\n",
    "def calc_is_correct(gt_bboxes, pred_bboxes):\n",
    "    \"\"\"\n",
    "    gt_bboxes: (N, 4) np.array in xywh format\n",
    "    pred_bboxes: (N, 5) np.array in conf+xywh format\n",
    "    \"\"\"\n",
    "    if len(gt_bboxes) == 0 and len(pred_bboxes) == 0:\n",
    "        tps, fps, fns = 0, 0, 0\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    elif len(gt_bboxes) == 0:\n",
    "        tps, fps, fns = 0, len(pred_bboxes), 0\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    elif len(pred_bboxes) == 0:\n",
    "        tps, fps, fns = 0, 0, len(gt_bboxes)\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    pred_bboxes = pred_bboxes[pred_bboxes[:,0].argsort()[::-1]] # sort by conf\n",
    "    \n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for iou_th in np.arange(0.3, 0.85, 0.05):\n",
    "        tp, fp, fn = calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th)\n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "    return tps, fps, fns\n",
    "\n",
    "def calc_f2_score(gt_bboxes_list, pred_bboxes_list, verbose=False):\n",
    "    \"\"\"\n",
    "    gt_bboxes_list: list of (N, 4) np.array in xywh format\n",
    "    pred_bboxes_list: list of (N, 5) np.array in conf+xywh format\n",
    "    \"\"\"\n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for gt_bboxes, pred_bboxes in zip(gt_bboxes_list, pred_bboxes_list):\n",
    "        tp, fp, fn = calc_is_correct(gt_bboxes, pred_bboxes)\n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "        if verbose:\n",
    "            num_gt = len(gt_bboxes)\n",
    "            num_pred = len(pred_bboxes)\n",
    "            print(f'num_gt:{num_gt:<3} num_pred:{num_pred:<3} tp:{tp:<3} fp:{fp:<3} fn:{fn:<3}')\n",
    "            \n",
    "    print(f'tps:{tps}  fp:{fps}  fn:{fns}')\n",
    "    return f_beta(tps, fps, fns, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "859e47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, conf=0.25, iou=0.50):\n",
    "    model = torch.hub.load('./',\n",
    "                           'custom',\n",
    "                           path=ckpt_path,\n",
    "                           source='local',\n",
    "                           force_reload=True)  # local repo\n",
    "    model.conf = conf  # NMS confidence threshold\n",
    "    model.iou  = iou  # NMS IoU threshold\n",
    "    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n",
    "    model.multi_label = False  # NMS multiple labels per box\n",
    "    model.max_det = 1000  # maximum number of detections per image\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, img_path, size=768, augment=False):\n",
    "    img = load_image(img_path)\n",
    "    height, width = img.shape[:2]\n",
    "    results = model(img, size=size, augment=augment)  # custom inference size\n",
    "    preds   = results.pandas().xyxy[0]\n",
    "    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n",
    "    if len(bboxes):\n",
    "        bboxes  = voc2coco(bboxes,height,width).astype(int)\n",
    "        confs   = preds.confidence.values\n",
    "        return bboxes, confs\n",
    "    else:\n",
    "        return [],[]\n",
    "    \n",
    "def format_prediction(bboxes, confs):\n",
    "    annot = ''\n",
    "    if len(bboxes)>0:\n",
    "        for idx in range(len(bboxes)):\n",
    "            xmin, ymin, w, h = bboxes[idx]\n",
    "            conf             = confs[idx]\n",
    "            annot += f'{conf} {xmin} {ymin} {w} {h}'\n",
    "            annot +=' '\n",
    "        annot = annot.strip(' ')\n",
    "    return annot\n",
    "\n",
    "def show_img(img, bboxes, name, color, bbox_format='yolo'):\n",
    "    names  = [name]*len(bboxes)\n",
    "    labels = [0]*len(bboxes)\n",
    "    img    = draw_bboxes(img = img,\n",
    "                           bboxes = bboxes, \n",
    "                           classes = names,\n",
    "                           class_ids = labels,\n",
    "                           class_name = True, \n",
    "                           colors = color, \n",
    "                           bbox_format = bbox_format,\n",
    "                           line_thickness = 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736fe4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    15269\n",
       "val       8232\n",
       "Name: fold, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA  = '../input/yolov5_public_fold1/train.csv'\n",
    "ROOT  = './starfish/yolov5s6_e5_bs2_lr01_img3600_public_fold1/'\n",
    "CKPT_PATH = ROOT+'weights/best.pt'\n",
    "IMG_SIZE  = 3600\n",
    "CONF      = 0.25\n",
    "IOU       = 0.45\n",
    "MAX_DET   = 1000\n",
    "AUGMENT   = False\n",
    "\n",
    "pd.read_csv(DATA).fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b755392a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 2021-12-31 torch 1.10.1 CUDA:0 (Quadro RTX 6000, 24220MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.45\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('../yolov5/',\n",
    "                       'custom',\n",
    "                       path=CKPT_PATH,\n",
    "                       source='local',\n",
    "                       force_reload=True\n",
    "                      )\n",
    "print(model.conf)\n",
    "print(model.iou)\n",
    "print(model.max_det)\n",
    "\n",
    "model.conf = CONF  # NMS confidence threshold\n",
    "model.iou  = IOU  # NMS IoU threshold\n",
    "model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n",
    "model.multi_label = False  # NMS multiple labels per box\n",
    "model.max_det = MAX_DET  # maximum number of detections per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae5963b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077aaadb39294b20b19dcff0e16e82cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345bbe6dbb294af789f45070527eb1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA)\n",
    "df['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\n",
    "df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\n",
    "val = df[df.fold == 'val'].reset_index(drop=True)\n",
    "val['bboxes'] = val['bboxes'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4fa425",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val.sort_values(['video_id', 'video_frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a177db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, df, **kw):\n",
    "    # output video config\n",
    "    fps = 24 # don't know exact value\n",
    "    width = 1280\n",
    "    height = 720\n",
    "    video_path = ROOT+\"output.mp4\"\n",
    "    output_video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, (1280, height))\n",
    "    \n",
    "    gt_bbox_list = []\n",
    "    pr_bbox_list = []\n",
    "    pr_conf_list = []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        pred_bbox, pred_conf = predict(model, row.image_path, IMG_SIZE, AUGMENT)\n",
    "        if len(pred_conf) > 0:\n",
    "            pred = np.concatenate((pred_conf[..., None], pred_bbox), axis=1)\n",
    "        else:\n",
    "            pred = np.array([])\n",
    "            \n",
    "        gt = np.array(row.bboxes)\n",
    "        gt_bbox_list.append(gt)\n",
    "        pr_bbox_list.append(pred)\n",
    "        \n",
    "        pr_conf_list.append(pred_conf)\n",
    "        \n",
    "        img = load_image(row.image_path)\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        c1 = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n",
    "        c2 = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n",
    "        img = show_img(img, pred_bbox, 'PRED', c1, 'coco')\n",
    "        img = show_img(img, gt, 'GT', c2, 'coco')\n",
    "        output_video.write(img)\n",
    "        \n",
    "        #if i > 100:\n",
    "        #    break\n",
    "        \n",
    "    # perform iou and conf search for best f2\n",
    "    \n",
    "    \n",
    "    score = calc_f2_score(gt_bbox_list, pr_bbox_list, verbose=False)\n",
    "    print(f\"F2 score {score: .5f}\")\n",
    "    print(f\"Video save in {video_path}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36536d97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6147d2735d4f6bb3860c3a50be159c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kunhao/anaconda/envs/pytorch-37/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576dce43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
