{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedfff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Copy from implement by \n",
    "# https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/290083\n",
    "# https://www.kaggle.com/bamps53/competition-metric-implementation?scriptVersionId=81087805\n",
    "\n",
    "# Yolo infer modified from \n",
    "# https://www.kaggle.com/awsaf49/great-barrier-reef-yolov5-infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102951b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 30, 30\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "from ast import literal_eval\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "from norfair import Detection, Tracker\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('../input/tensorflow-great-barrier-reef')\n",
    "import torch\n",
    "from PIL import Image\n",
    "import ast\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import *\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534c320",
   "metadata": {},
   "source": [
    "# Utils func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ea90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(row):\n",
    "    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n",
    "    return row\n",
    "\n",
    "def voc2yolo(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    voc  => [x1, y1, x2, y2]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    w = bboxes[..., 2] - bboxes[..., 0]\n",
    "    h = bboxes[..., 3] - bboxes[..., 1]\n",
    "    \n",
    "    bboxes[..., 0] = bboxes[..., 0] + w/2\n",
    "    bboxes[..., 1] = bboxes[..., 1] + h/2\n",
    "    bboxes[..., 2] = w\n",
    "    bboxes[..., 3] = h\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2voc(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    voc  => [x1, y1, x2, y2]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def coco2yolo(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # normolizinig\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    # converstion (xmin, ymin) => (xmid, ymid)\n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2coco(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # denormalizing\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    # converstion (xmid, ymid) => (xmin, ymin) \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def voc2coco(bboxes, image_height=720, image_width=1280):\n",
    "    bboxes  = voc2yolo(bboxes, image_height, image_width)\n",
    "    bboxes  = yolo2coco(bboxes, image_height, image_width)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def coco2voc(bboxes, image_height=720, image_width=1280):\n",
    "    bboxes  = coco2yolo(bboxes, image_height, image_width)\n",
    "    bboxes  = yolo2voc(bboxes, image_height, image_width)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, \n",
    "                bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n",
    "     \n",
    "    image = img.copy()\n",
    "    show_classes = classes if show_classes is None else show_classes\n",
    "    colors = (0, 255 ,0) if colors is None else colors\n",
    "    \n",
    "    if bbox_format == 'yolo':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:\n",
    "            \n",
    "                x1 = round(float(bbox[0])*image.shape[1])\n",
    "                y1 = round(float(bbox[1])*image.shape[0])\n",
    "                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n",
    "                h  = round(float(bbox[3])*image.shape[0]/2)\n",
    "\n",
    "                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(get_label(cls)),\n",
    "                             line_thickness = line_thickness)\n",
    "            \n",
    "    elif bbox_format == 'coco':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:            \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                w  = int(round(bbox[2]))\n",
    "                h  = int(round(bbox[3]))\n",
    "\n",
    "                voc_bbox = (x1, y1, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness)\n",
    "\n",
    "    elif bbox_format == 'voc':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes: \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                x2 = int(round(bbox[2]))\n",
    "                y2 = int(round(bbox[3]))\n",
    "                voc_bbox = (x1, y1, x2, y2)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness)\n",
    "    else:\n",
    "        raise ValueError('wrong bbox format')\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes\n",
    "\n",
    "def get_imgsize(row):\n",
    "    row['width'], row['height'] = imagesize.get(row['image_path'])\n",
    "    return row\n",
    "\n",
    "np.random.seed(32)\n",
    "colors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n",
    "          for idx in range(1)]\n",
    "\n",
    "\n",
    "def decode_annotations(annotaitons_str):\n",
    "    \"\"\"decode annotations in string to list of dict\"\"\"\n",
    "    return literal_eval(annotaitons_str)\n",
    "\n",
    "def load_image_with_annotations(video_id, video_frame, image_dir, annotaitons_str):\n",
    "    img = load_image(video_id, video_frame, image_dir)\n",
    "    annotations = decode_annotations(annotaitons_str)\n",
    "    if len(annotations) > 0:\n",
    "        for ann in annotations:\n",
    "            cv2.rectangle(img, (ann['x'], ann['y']),\n",
    "                (ann['x'] + ann['width'], ann['y'] + ann['height']),\n",
    "                (255, 0, 0), thickness=2,)\n",
    "    return img\n",
    "\n",
    "def draw_predictions(img, pred_bboxes):\n",
    "    img = img.copy()\n",
    "    if len(pred_bboxes) > 0:\n",
    "        for bbox in pred_bboxes:\n",
    "            conf = bbox[0]\n",
    "            x, y, w, h = bbox[1:].round().astype(int)\n",
    "            cv2.rectangle(img, (x, y),\n",
    "                (x+w, y+h),\n",
    "                (0, 255, 255), thickness=2,)\n",
    "            cv2.putText(\n",
    "                img,\n",
    "                f\"{conf:.2}\",\n",
    "                (x, max(0, y-5)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 0, 255),\n",
    "                thickness=1,\n",
    "            )\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a9828",
   "metadata": {},
   "source": [
    "# CLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80844f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegHead(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(RegHead, self).__init__()\n",
    "        #self.norm = nn.LayerNorm(in_feat)\n",
    "        self.fc1 = nn.Linear(in_feat, out_feat)\n",
    "        #self.fc2 = nn.Linear(in_feat//2, out_feat)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.norm(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        #x = F.relu_(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = x.sigmoid() * 100.\n",
    "        return x\n",
    "\n",
    "class PawModel(nn.Module):\n",
    "    def __init__(self, arch='tf_efficientnet_b0_ns', pretrained=True, n_class=1):\n",
    "        super(PawModel, self).__init__()\n",
    "        \n",
    "        self.model = timm.create_model(arch, pretrained=pretrained)\n",
    "        \n",
    "        #print(self.model); assert False\n",
    "        \n",
    "        try:\n",
    "            if 'efficientnet' in  arch:\n",
    "                self.fc = RegHead(self.model.classifier.in_features, 1)\n",
    "                '''\n",
    "                for k, v in self.model.named_parameters():\n",
    "                    if 'blocks.4' in k or 'blocks.5' in k or 'blocks.6' in k:\n",
    "                        print(f'{k} unfrozen')\n",
    "                        v.requires_grad = True\n",
    "                    else:\n",
    "                        v.requires_grad = False\n",
    "                '''        \n",
    "                self.model.classifier = nn.Identity()\n",
    "            elif 'swin' in arch or 'vit' in arch:\n",
    "                self.fc = RegHead(self.model.head.in_features, n_class) #RegHead(self.model.head.in_features, 1)\n",
    "                \n",
    "                '''\n",
    "                # 11-17\n",
    "                \n",
    "                unfreeze_kws = ['layers.2.blocks.16', 'layers.2.blocks.17', 'layers.2.downsample', 'layers.3']\n",
    "\n",
    "                for k, v in self.model.named_parameters():\n",
    "                    if any([kw in k for kw in unfreeze_kws]):\n",
    "                        #print(f'{k} unfrozen')\n",
    "                        v.requires_grad = True\n",
    "                    else:\n",
    "                        v.requires_grad = False\n",
    "                '''  \n",
    "                self.model.head = nn.Identity()\n",
    "            else:\n",
    "                self.fc = RegHead(self.model.fc.in_features, 1)\n",
    "                self.model.fc = nn.Identity()\n",
    "                \n",
    "        except:\n",
    "            print(self.model)\n",
    "            #print(self.model.layers[3].blocks[1].mlp.fc2)\n",
    "            assert False\n",
    "            \n",
    "    def forward(self, imgs):\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        x = self.model(imgs) # (b*d, img_f)\n",
    "            \n",
    "        #x = F.normalize(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a68d061",
   "metadata": {},
   "source": [
    "# Metric Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42955d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(bboxes1, bboxes2, bbox_mode='xywh'):\n",
    "    # xmin ymin weight height\n",
    "    assert len(bboxes1.shape) == 2 and bboxes1.shape[1] == 4\n",
    "    assert len(bboxes2.shape) == 2 and bboxes2.shape[1] == 4\n",
    "    \n",
    "    bboxes1 = bboxes1.copy()\n",
    "    bboxes2 = bboxes2.copy()\n",
    "    \n",
    "    if bbox_mode == 'xywh':\n",
    "        bboxes1[:, 2:] += bboxes1[:, :2]\n",
    "        bboxes2[:, 2:] += bboxes2[:, :2]\n",
    "\n",
    "    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n",
    "    xA = np.maximum(x11, np.transpose(x21))\n",
    "    yA = np.maximum(y11, np.transpose(y21))\n",
    "    xB = np.minimum(x12, np.transpose(x22))\n",
    "    yB = np.minimum(y12, np.transpose(y22))\n",
    "    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n",
    "    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n",
    "    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n",
    "    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n",
    "    return iou\n",
    "\n",
    "def f_beta(tp, fp, fn, beta=2):\n",
    "    return (1+beta**2)*tp / ((1+beta**2)*tp + beta**2*fn+fp+1e-6)\n",
    "\n",
    "def calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th, verbose=False):\n",
    "    gt_bboxes = gt_bboxes.copy()\n",
    "    pred_bboxes = pred_bboxes.copy()\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for k, pred_bbox in enumerate(pred_bboxes): # fixed in ver.7\n",
    "        ious = calc_iou(gt_bboxes, pred_bbox[None, 1:])\n",
    "        max_iou = ious.max()\n",
    "        if max_iou > iou_th:\n",
    "            tp += 1\n",
    "            gt_bboxes = np.delete(gt_bboxes, ious.argmax(), axis=0)\n",
    "        else:\n",
    "            fp += 1\n",
    "        if len(gt_bboxes) == 0:\n",
    "            fp += len(pred_bboxes) - (k + 1) # fix in ver.7\n",
    "            break\n",
    "\n",
    "    fn = len(gt_bboxes)\n",
    "    return tp, fp, fn\n",
    "\n",
    "def calc_is_correct(gt_bboxes, pred_bboxes):\n",
    "    \"\"\"\n",
    "    gt_bboxes: (N, 4) np.array in xywh format\n",
    "    pred_bboxes: (N, 5) np.array in conf+xywh format\n",
    "    \"\"\"\n",
    "    if len(gt_bboxes) == 0 and len(pred_bboxes) == 0:\n",
    "        tps, fps, fns = 0, 0, 0\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    elif len(gt_bboxes) == 0:\n",
    "        tps, fps, fns = 0, len(pred_bboxes), 0\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    elif len(pred_bboxes) == 0:\n",
    "        tps, fps, fns = 0, 0, len(gt_bboxes)\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    pred_bboxes = pred_bboxes[pred_bboxes[:,0].argsort()[::-1]] # sort by conf\n",
    "    \n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for iou_th in np.arange(0.3, 0.85, 0.05):\n",
    "        tp, fp, fn = calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th)\n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "    return tps, fps, fns\n",
    "\n",
    "def calc_f2_score(gt_bboxes_list, pred_bboxes_list, verbose=False):\n",
    "    \"\"\"\n",
    "    gt_bboxes_list: list of (N, 4) np.array in xywh format\n",
    "    pred_bboxes_list: list of (N, 5) np.array in conf+xywh format\n",
    "    \"\"\"\n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for gt_bboxes, pred_bboxes in zip(gt_bboxes_list, pred_bboxes_list):\n",
    "        tp, fp, fn = calc_is_correct(gt_bboxes, pred_bboxes)\n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "        if verbose:\n",
    "            num_gt = len(gt_bboxes)\n",
    "            num_pred = len(pred_bboxes)\n",
    "            print(f'num_gt:{num_gt:<3} num_pred:{num_pred:<3} tp:{tp:<3} fp:{fp:<3} fn:{fn:<3}')\n",
    "            \n",
    "    print(f'tps:{tps}  fp:{fps}  fn:{fns}')\n",
    "    return f_beta(tps, fps, fns, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b810ea0",
   "metadata": {},
   "source": [
    "# Yolo & Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "859e47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model(ckpt_path, conf=None, iou=None, max_det=None):\n",
    "    model = torch.hub.load('./',\n",
    "                           'custom',\n",
    "                           path=ckpt_path,\n",
    "                           source='local',\n",
    "                           force_reload=True)  # local repo\n",
    "    if conf is not None:\n",
    "        model.conf = conf  # NMS confidence threshold\n",
    "    if iou is not None:\n",
    "        model.iou  = iou  # NMS IoU threshold\n",
    "    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n",
    "    model.multi_label = False  # NMS multiple labels per box\n",
    "    if model.max_det is not None:\n",
    "        model.max_det = max_det  # maximum number of detections per image\n",
    "    return model\n",
    "\n",
    "\n",
    "def yolo_predict(model, img, conf, size=3600, augment=False):\n",
    "    height, width = img.shape[:2]\n",
    "    results = model(img, size=size, augment=augment)  # custom inference size\n",
    "    preds   = results.pandas().xyxy[0]\n",
    "    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n",
    "    confs   = preds.confidence.values\n",
    "    keep = confs > conf\n",
    "    # voc format : 'xmin','ymin','xmax','ymax'\n",
    "    return bboxes[keep], confs[keep]\n",
    "\n",
    "    \n",
    "def tracking_predict(tracker, frame_id, bboxes, scores):\n",
    "    \n",
    "    def to_norfair(detects, scores, frame_id):\n",
    "        result = []\n",
    "        for (x_min, y_min, x_max, y_max), score in zip(detects, scores):\n",
    "            xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "            w, h = x_max - x_min, y_max - y_min\n",
    "            result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n",
    "\n",
    "        return result\n",
    "    \n",
    "    track_bboxes = []\n",
    "    track_scores = []\n",
    "    \n",
    "    tracked_objects = tracker.update(detections=to_norfair(bboxes, scores, frame_id))\n",
    "    for tobj in tracked_objects:\n",
    "        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n",
    "        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n",
    "            continue\n",
    "        xc, yc = tobj.estimate[0]\n",
    "        score = tobj.last_detection.scores[0]\n",
    "        \n",
    "        track_bboxes.append([xc, yc, bbox_width, bbox_height])\n",
    "        track_scores.append(score)\n",
    "    track_bboxes = np.array(track_bboxes)\n",
    "    track_scores = np.array(track_scores)\n",
    "    \n",
    "    if len(track_scores) == 0:\n",
    "        return bboxes, scores\n",
    "    \n",
    "    track_bboxes[:, 0] = track_bboxes[:, 0] - track_bboxes[:, 2]/2\n",
    "    track_bboxes[:, 1] = track_bboxes[:, 1] - track_bboxes[:, 3]/2\n",
    "    track_bboxes = coco2voc(track_bboxes)\n",
    "\n",
    "    if len(scores) == 0:\n",
    "        return track_bboxes, track_scores\n",
    "\n",
    "    bboxes = np.concatenate([bboxes, np.array(track_bboxes)], 0)\n",
    "    scores = np.concatenate([scores, np.array(track_scores)], 0)\n",
    "\n",
    "    return bboxes, scores\n",
    "\n",
    "\n",
    "def cls_predict(cls_model, img, crops):\n",
    "    \n",
    "    conf = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for crop in crops:\n",
    "            \n",
    "            x, y, x2, y2 = crop.astype(int)\n",
    "            \n",
    "            img_ = img[y:y2, x:x2, :]\n",
    "            #print(\"crop\", crop, img.shape, img_.shape)\n",
    "            img_pt = tfm(image=img_)['image'].unsqueeze(0).to('cuda:0')\n",
    "            p = cls_model(img_pt)[0].sigmoid().detach().cpu().numpy()[0]\n",
    "            conf += [p]\n",
    "            \n",
    "    return np.array(conf)\n",
    "\n",
    "    \n",
    "def format_prediction(bboxes, confs):\n",
    "    annot = ''\n",
    "    if len(bboxes)>0:\n",
    "        for idx in range(len(bboxes)):\n",
    "            xmin, ymin, xmax, ymax = bboxes[idx]\n",
    "            w = int(xmax - xmin)\n",
    "            h = int(ymax - ymin)\n",
    "            conf             = confs[idx]\n",
    "            annot += f'{conf} {xmin} {ymin} {w} {h}'\n",
    "            annot +=' '\n",
    "        annot = annot.strip(' ')\n",
    "    return annot\n",
    "\n",
    "\n",
    "def show_img(img, bboxes, name, color, bbox_format='voc'):\n",
    "    names  = [name]*len(bboxes)\n",
    "    labels = [0]*len(bboxes)\n",
    "    img    = draw_bboxes(img = img,\n",
    "                           bboxes = bboxes, \n",
    "                           classes = names,\n",
    "                           class_ids = labels,\n",
    "                           class_name = True, \n",
    "                           colors = color, \n",
    "                           bbox_format = bbox_format,\n",
    "                           line_thickness = 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4467a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(detection, tracked_object):\n",
    "    return np.linalg.norm(detection.points - tracked_object.estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c7d9c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f2_score_v2(gt_bboxes_list, pred_bboxes_list, verbose=False):\n",
    "    \"\"\"\n",
    "    gt_bboxes_list: list of (N, 4) np.array in xywh format\n",
    "    pred_bboxes_list: list of (N, 5) np.array in conf+xywh format\n",
    "    \"\"\"\n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for gt_bboxes, pred_bboxes in zip(gt_bboxes_list, pred_bboxes_list):\n",
    "        tp, fp, fn = calc_is_correct(gt_bboxes, pred_bboxes)\n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "        if verbose:\n",
    "            num_gt = len(gt_bboxes)\n",
    "            num_pred = len(pred_bboxes)\n",
    "            print(f'num_gt:{num_gt:<3} num_pred:{num_pred:<3} tp:{tp:<3} fp:{fp:<3} fn:{fn:<3}')\n",
    "    \n",
    "    precision = tps/(tps+fps+0.1)\n",
    "    recall = tps/(tps+fns+0.1)\n",
    "\n",
    "    f2 = f_beta(tps, fps, fns, beta=2)\n",
    "\n",
    "    print(f'tps:{tps}  fps:{fps}  fns:{fns}, p: {precision:.3f}, r: {recall:.3f}, f2: {f2:.3f}')\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c2b78a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_area(b):\n",
    "    return (b[:, 2]-b[:, 0]).astype(int) * (b[:, 3]-b[:, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b5755a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ilegal_bbox(pred_bbox, pred_conf):\n",
    "    pred_bbox[:, 0] = np.clip(pred_bbox[:, 0], 0, 1200)\n",
    "    pred_bbox[:, 2] = np.clip(pred_bbox[:, 2], 0, 1200)\n",
    "    pred_bbox[:, 1] = np.clip(pred_bbox[:, 1], 0, 720)\n",
    "    pred_bbox[:, 3] = np.clip(pred_bbox[:, 3], 0, 720)\n",
    "    pred_bbox = np.clip(pred_bbox, 0, 1200)\n",
    "    area = get_bboxes_area(pred_bbox)\n",
    "    keep = np.logical_and(area > MIN_AREA, area < MAX_AREA)\n",
    "    pred_bbox = pred_bbox[keep]\n",
    "    pred_conf = pred_conf[keep]\n",
    "    \n",
    "    return pred_bbox, pred_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09a177db",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n",
    "c2 = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n",
    "\n",
    "\n",
    "def evaluate(model, cls_model, tracker, df, name, **kw):\n",
    "    # output video config\n",
    "    frame_id = 0\n",
    "    fps = 24 # don't know exact value\n",
    "    width = 1280\n",
    "    height = 720\n",
    "    video_path = ROOT+name\n",
    "    print(\"Video save in :\", video_path)\n",
    "    output_video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, (1280, height))\n",
    "    \n",
    "    gt_bbox_list = []\n",
    "    pr_bbox_list = []\n",
    "    pr_conf_list = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        image = load_image(row.image_path)\n",
    "        pred_bbox, pred_conf = yolo_predict(model, image, INFER_CONF, IMG_SIZE, AUGMENT)\n",
    "        if tracker is not None:\n",
    "            pred_bbox, pred_conf = tracking_predict(tracker, frame_id, pred_bbox, pred_conf)\n",
    "        \n",
    "        pred_bbox, pred_conf = process_ilegal_bbox(pred_bbox, pred_conf)\n",
    "        #print(len(pred_bbox), pred_bbox)\n",
    "        if len(pred_conf) > 0:\n",
    "            if cls_model is not None:\n",
    "                # print(image.shape, pred_bbox)\n",
    "                cls_confs = cls_predict(cls_model, image, pred_bbox)\n",
    "                filt = cls_confs > CLS_CONF\n",
    "                pred_bbox = pred_bbox[filt]\n",
    "                pred_conf = pred_conf[filt]\n",
    "            pred_bbox = voc2coco(pred_bbox)\n",
    "            pred = np.concatenate((pred_conf[..., None], pred_bbox), axis=1)\n",
    "        else:\n",
    "            pred = np.array([])\n",
    "            \n",
    "        gt = np.array(row.bboxes)\n",
    "        gt_bbox_list.append(gt)\n",
    "        \n",
    "        pr_bbox_list.append(pred)\n",
    "        pr_conf_list.append(pred_conf)\n",
    "#         print(\"GT\", gt)\n",
    "#         print(\"PR\", pred)\n",
    "#         print(\"-\"*50)\n",
    "        \n",
    "        img = load_image(row.image_path)[:,:,::-1]\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        img = show_img(img, pred_bbox, 'PRED', c1, 'coco')\n",
    "        img = show_img(img, gt, 'GT', c2, 'coco')\n",
    "        output_video.write(img)\n",
    "        plt.close()\n",
    "        frame_id += 1\n",
    "        #if i > 100:\n",
    "        #    break\n",
    "\n",
    "    # perform iou and conf search for best f2\n",
    "    \n",
    "    \n",
    "    score = calc_f2_score_v2(gt_bbox_list, pr_bbox_list, verbose=False)\n",
    "    print(f\"F2 score {score: .5f}\")\n",
    "    print(f\"Video save in {video_path}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "67557012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 4b1ec65 torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video save in : ./cv/yolov5s6_e5_bs2_lr01_img5400_public_video1_conf0.05_.mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba380b63df4c47438ea557a902a11cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps:43865  fps:54252  fns:25329, p: 0.447, r: 0.634, f2: 0.585\n",
      "F2 score  0.58503\n",
      "Video save in ./cv/yolov5s6_e5_bs2_lr01_img5400_public_video1_conf0.05_.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.585033596251237"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm = A.Compose([A.Resize(224, 224, interpolation=cv2.INTER_LANCZOS4),      \n",
    "                A.Normalize(mean=[0], std=[1], max_pixel_value=255.0, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "device = 'cuda:0'\n",
    "cls_model = PawModel('swin_large_patch4_window7_224', pretrained=False, n_class=1).to(device)\n",
    "ckpt = './checkpoints/swin224-cls-fold_1_ep_2'\n",
    "cls_model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "cls_model.eval()\n",
    "\n",
    "\n",
    "ROOT  = './cv/'\n",
    "Path(ROOT).mkdir(exist_ok=True, parents=True)\n",
    "CKPT_PATH = './starfish/yolov5s6_e5_bs2_img5400_video1_HRv1.1_01-21/weights/yolov5s6_e5_bs2_lr01_img5400_public_video1.pt'\n",
    "IMG_SIZE  = 5400\n",
    "MNS_CONF      = 0.01  # 0.25\n",
    "MNS_IOU       = None  # 0.45\n",
    "INFER_CONF = 0.01\n",
    "CLS_CONF   = 0.9\n",
    "MAX_DET   = 1000  #1000\n",
    "AUGMENT   = False\n",
    "MIN_AREA  = 240\n",
    "MAX_AREA  = 20000\n",
    "VIDEO = 1\n",
    "\n",
    "DATA  = f'../input/yolov5_video{VIDEO}/train.csv'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "df = pd.read_csv(DATA)\n",
    "df['annotations'] = df['annotations'].apply(lambda x: ast.literal_eval(x))\n",
    "df['num_bbox'] = df['annotations'].apply(lambda x: len(x))\n",
    "val = df[df.fold == 'val'].reset_index(drop=True)\n",
    "val['bboxes'] = val['bboxes'].apply(eval)\n",
    "val = val.sort_values(['video_id', 'video_frame'])\n",
    "\n",
    "model = load_yolo_model(CKPT_PATH, MNS_CONF, MNS_IOU, MAX_DET)\n",
    "tracker = Tracker(\n",
    "    distance_function=euclidean_distance, \n",
    "    distance_threshold=30,\n",
    "    hit_inertia_min=3,\n",
    "    hit_inertia_max=6,\n",
    "    initialization_delay=1,\n",
    ")\n",
    "video_name = CKPT_PATH.split(\"/\")[-1][:-3]+ \"_conf0.05_\"+\".mp4\"\n",
    "evaluate(model, cls_model, tracker, val, video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299b6ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783babf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
