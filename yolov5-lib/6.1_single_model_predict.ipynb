{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e65416",
   "metadata": {},
   "source": [
    "# evaluate 4 stage\n",
    "\n",
    "**1. Model predict**\n",
    "2. Model ensemble\n",
    "3. Tracking\n",
    "4. Post classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811a261",
   "metadata": {},
   "source": [
    "# libs & utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b2a0b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1aa0fcb95f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 30, 30\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "from ast import literal_eval\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "from norfair import Detection, Tracker\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('../input/tensorflow-great-barrier-reef')\n",
    "import torch\n",
    "from PIL import Image\n",
    "import ast\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import *\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(row):\n",
    "    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n",
    "    return row\n",
    "\n",
    "def voc2yolo(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    voc  => [x1, y1, x2, y2]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    w = bboxes[..., 2] - bboxes[..., 0]\n",
    "    h = bboxes[..., 3] - bboxes[..., 1]\n",
    "    \n",
    "    bboxes[..., 0] = bboxes[..., 0] + w/2\n",
    "    bboxes[..., 1] = bboxes[..., 1] + h/2\n",
    "    bboxes[..., 2] = w\n",
    "    bboxes[..., 3] = h\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2voc(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    voc  => [x1, y1, x2, y2]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def coco2yolo(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # normolizinig\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    # converstion (xmin, ymin) => (xmid, ymid)\n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2coco(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # denormalizing\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    # converstion (xmid, ymid) => (xmin, ymin) \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def voc2coco(bboxes, image_height=720, image_width=1280):\n",
    "    bboxes  = voc2yolo(bboxes, image_height, image_width)\n",
    "    bboxes  = yolo2coco(bboxes, image_height, image_width)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def coco2voc(bboxes, image_height=720, image_width=1280):\n",
    "    bboxes  = coco2yolo(bboxes, image_height, image_width)\n",
    "    bboxes  = yolo2voc(bboxes, image_height, image_width)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, \n",
    "                bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n",
    "     \n",
    "    image = img.copy()\n",
    "    show_classes = classes if show_classes is None else show_classes\n",
    "    colors = (0, 255 ,0) if colors is None else colors\n",
    "    \n",
    "    if bbox_format == 'yolo':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:\n",
    "            \n",
    "                x1 = round(float(bbox[0])*image.shape[1])\n",
    "                y1 = round(float(bbox[1])*image.shape[0])\n",
    "                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n",
    "                h  = round(float(bbox[3])*image.shape[0]/2)\n",
    "\n",
    "                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(get_label(cls)),\n",
    "                             line_thickness = line_thickness)\n",
    "            \n",
    "    elif bbox_format == 'coco':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:            \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                w  = int(round(bbox[2]))\n",
    "                h  = int(round(bbox[3]))\n",
    "\n",
    "                voc_bbox = (x1, y1, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness)\n",
    "\n",
    "    elif bbox_format == 'voc':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes: \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                x2 = int(round(bbox[2]))\n",
    "                y2 = int(round(bbox[3]))\n",
    "                voc_bbox = (x1, y1, x2, y2)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness)\n",
    "    else:\n",
    "        raise ValueError('wrong bbox format')\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes\n",
    "\n",
    "def get_imgsize(row):\n",
    "    row['width'], row['height'] = imagesize.get(row['image_path'])\n",
    "    return row\n",
    "\n",
    "np.random.seed(32)\n",
    "colors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n",
    "          for idx in range(1)]\n",
    "\n",
    "\n",
    "def decode_annotations(annotaitons_str):\n",
    "    \"\"\"decode annotations in string to list of dict\"\"\"\n",
    "    return literal_eval(annotaitons_str)\n",
    "\n",
    "def load_image_with_annotations(video_id, video_frame, image_dir, annotaitons_str):\n",
    "    img = load_image(video_id, video_frame, image_dir)\n",
    "    annotations = decode_annotations(annotaitons_str)\n",
    "    if len(annotations) > 0:\n",
    "        for ann in annotations:\n",
    "            cv2.rectangle(img, (ann['x'], ann['y']),\n",
    "                (ann['x'] + ann['width'], ann['y'] + ann['height']),\n",
    "                (255, 0, 0), thickness=2,)\n",
    "    return img\n",
    "\n",
    "def draw_predictions(img, pred_bboxes):\n",
    "    img = img.copy()\n",
    "    if len(pred_bboxes) > 0:\n",
    "        for bbox in pred_bboxes:\n",
    "            conf = bbox[0]\n",
    "            x, y, w, h = bbox[1:].round().astype(int)\n",
    "            cv2.rectangle(img, (x, y),\n",
    "                (x+w, y+h),\n",
    "                (0, 255, 255), thickness=2,)\n",
    "            cv2.putText(\n",
    "                img,\n",
    "                f\"{conf:.2}\",\n",
    "                (x, max(0, y-5)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 0, 255),\n",
    "                thickness=1,\n",
    "            )\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegHead(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(RegHead, self).__init__()\n",
    "        #self.norm = nn.LayerNorm(in_feat)\n",
    "        self.fc1 = nn.Linear(in_feat, out_feat)\n",
    "        #self.fc2 = nn.Linear(in_feat//2, out_feat)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.norm(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        #x = F.relu_(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = x.sigmoid() * 100.\n",
    "        return x\n",
    "\n",
    "class PawModel(nn.Module):\n",
    "    def __init__(self, arch='tf_efficientnet_b0_ns', pretrained=True, n_class=1):\n",
    "        super(PawModel, self).__init__()\n",
    "        \n",
    "        self.model = timm.create_model(arch, pretrained=pretrained)\n",
    "        \n",
    "        #print(self.model); assert False\n",
    "        \n",
    "        try:\n",
    "            if 'efficientnet' in  arch:\n",
    "                self.fc = RegHead(self.model.classifier.in_features, 1)\n",
    "                '''\n",
    "                for k, v in self.model.named_parameters():\n",
    "                    if 'blocks.4' in k or 'blocks.5' in k or 'blocks.6' in k:\n",
    "                        print(f'{k} unfrozen')\n",
    "                        v.requires_grad = True\n",
    "                    else:\n",
    "                        v.requires_grad = False\n",
    "                '''        \n",
    "                self.model.classifier = nn.Identity()\n",
    "            elif 'swin' in arch or 'vit' in arch:\n",
    "                self.fc = RegHead(self.model.head.in_features, n_class) #RegHead(self.model.head.in_features, 1)\n",
    "                \n",
    "                '''\n",
    "                # 11-17\n",
    "                \n",
    "                unfreeze_kws = ['layers.2.blocks.16', 'layers.2.blocks.17', 'layers.2.downsample', 'layers.3']\n",
    "\n",
    "                for k, v in self.model.named_parameters():\n",
    "                    if any([kw in k for kw in unfreeze_kws]):\n",
    "                        #print(f'{k} unfrozen')\n",
    "                        v.requires_grad = True\n",
    "                    else:\n",
    "                        v.requires_grad = False\n",
    "                '''  \n",
    "                self.model.head = nn.Identity()\n",
    "            else:\n",
    "                self.fc = RegHead(self.model.fc.in_features, 1)\n",
    "                self.model.fc = nn.Identity()\n",
    "                \n",
    "        except:\n",
    "            print(self.model)\n",
    "            #print(self.model.layers[3].blocks[1].mlp.fc2)\n",
    "            assert False\n",
    "            \n",
    "    def forward(self, imgs):\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        x = self.model(imgs) # (b*d, img_f)\n",
    "            \n",
    "        #x = F.normalize(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "def get_cls_embeddings(cls_model, img, crops):\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for crop in crops:\n",
    "            x, y, w, h = crop\n",
    "            img_ = img[y:y+h, x:x+w,:]\n",
    "            img_pt = tfm(image=img_)['image'].unsqueeze(0).to('cuda:0')\n",
    "            embed = cls_model(img_pt)[0].detach().cpu().numpy()\n",
    "            embeds += [embed]\n",
    "            \n",
    "    return np.array(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d959944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308ef80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
